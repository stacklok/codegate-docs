{/* This content is pulled out as an include because Prettier can't handle the indentation needed to get this to appear in the right spot under a list item. */}

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import LocalModelRecommendation from './_local-model-recommendation.md';

<Tabs groupId="provider" queryString="provider">
<TabItem value="mux" label="CodeGate muxing" default>
First, configure your [provider(s)](../features/muxing.mdx#add-a-provider) and
select a model for each of your
[workspace(s)](../features/workspaces.mdx#manage-workspaces) in the CodeGate dashboard.

Run aider with the OpenAI base URL set to `http://localhost:8989/v1/mux`. You
can do this with the `OPENAI_API_BASE` environment variable or on the command
line as shown below.

The `--openai-api-key` parameter is required but the value is not used. The
`--model` setting must start with `openai/` but the actual model is determined
by your CodeGate workspace.

```bash
aider --openai-api-base http://localhost:8989/v1/mux --openai-api-key fake-value-not-used --model openai/fake-value-not-used
```

</TabItem>
<TabItem value="ollama" label="Ollama">

You need Ollama installed on your local system with the server running
(`ollama serve`) to use this provider.

CodeGate connects to `http://host.docker.internal:11434` by default. If you
changed the default Ollama server port or to connect to a remote Ollama
instance, launch CodeGate with the `CODEGATE_OLLAMA_URL` environment variable
set to the correct URL. See [Configure CodeGate](../how-to/configure.md).

Before you run aider, set the Ollama base URL to CodeGate's API port using an
environment variable. Alternately, use one of aider's other
[supported configuration methods](https://aider.chat/docs/config/api-keys.html)
to set the corresponding values.

<Tabs groupId="os">
<TabItem value="macos" label="macOS / Linux" default>

```bash
export OLLAMA_API_BASE=http://localhost:8989/ollama
```

:::note

To persist this setting, add it to your shell profile (e.g., `~/.bashrc` or
`~/.zshrc`) or use one of's aider's other
[supported configuration methods](https://aider.chat/docs/config/api-keys.html).

:::

</TabItem>
<TabItem value="windows" label="Windows">

```bash
setx OLLAMA_API_BASE http://localhost:8989/ollama
```

:::note

Restart your shell after running `setx`.

:::

</TabItem>
</Tabs>

Then run aider:

```bash
aider --model ollama_chat/<MODEL_NAME>
```

Replace `<MODEL_NAME>` with the name of a coding model you have installed
locally using `ollama pull`.

<LocalModelRecommendation />

For more information, see the
[aider docs for connecting to Ollama](https://aider.chat/docs/llms/ollama.html).

</TabItem>
<TabItem value="openai" label="OpenAI">

You need an [OpenAI API](https://openai.com/api/) account to use this provider.
To use a different OpenAI-compatible endpoint, set the `CODEGATE_OPENAI_URL`
[configuration parameter](../how-to/configure.md#config-parameters).

Before you run aider, set environment variables for your API key and to set the
API base URL to CodeGate's API port. Alternately, use one of aider's other
[supported configuration methods](https://aider.chat/docs/config/api-keys.html)
to set the corresponding values.

<Tabs groupId="os">
<TabItem value="macos" label="macOS / Linux" default>

```bash
export OPENAI_API_KEY=<YOUR_API_KEY>
export OPENAI_API_BASE=http://localhost:8989/openai
```

:::note

To persist these variables, add them to your shell profile (e.g., `~/.bashrc` or
`~/.zshrc`).

:::

</TabItem>
<TabItem value="windows" label="Windows">

```bash
setx OPENAI_API_KEY <YOUR_API_KEY>
setx OPENAI_API_BASE http://localhost:8989/openai
```

:::note

Restart your shell after running `setx`.

:::

</TabItem>
</Tabs>

Replace `<YOUR_API_KEY>` with your
[OpenAI API key](https://platform.openai.com/api-keys).

Then run `aider` as normal. For more information, see the
[aider docs for connecting to OpenAI](https://aider.chat/docs/llms/openai.html).

</TabItem>
</Tabs>
